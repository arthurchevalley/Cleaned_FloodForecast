{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u7zaCnT4hge"
      },
      "source": [
        "# General Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HhngaAr3xdd"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "from warnings import simplefilter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXf8sMiA35qt"
      },
      "outputs": [],
      "source": [
        "simplefilter(\"ignore\")\n",
        "\n",
        "# Set Matplotlib defaults\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True, figsize=(11, 4))\n",
        "plt.rc(\n",
        "    \"axes\",\n",
        "    labelweight=\"bold\",\n",
        "    labelsize=\"large\",\n",
        "    titleweight=\"bold\",\n",
        "    titlesize=16,\n",
        "    titlepad=10,\n",
        ")\n",
        "plot_params = dict(\n",
        "    color=\"0.75\",\n",
        "    style=\".-\",\n",
        "    markeredgecolor=\"0.25\",\n",
        "    markerfacecolor=\"0.25\",\n",
        ")\n",
        "\n",
        "plot_params2 = dict(\n",
        "    color=\"red\",\n",
        "    style=\".-\",\n",
        "    markeredgecolor=\"0.25\",\n",
        "    markerfacecolor=\"0.25\",\n",
        ")\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "\n",
        "def plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n",
        "    palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n",
        "    if palette_kwargs is not None:\n",
        "        palette_kwargs_.update(palette_kwargs)\n",
        "    palette = sns.color_palette(**palette_kwargs_)\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    ax.set_prop_cycle(plt.cycler('color', palette))\n",
        "    for date, preds in y[::every].iterrows():\n",
        "        preds.index = pd.period_range(start=date, periods=len(preds), freq='H')\n",
        "        preds.plot(ax=ax)\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_-YAUHx39YL"
      },
      "source": [
        "# Dataset Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oicsLFZr4EJi"
      },
      "source": [
        "### Water Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_snaLmX3_NZ"
      },
      "outputs": [],
      "source": [
        "def prepare_df(dataf):\n",
        "  dataf = (dataf\n",
        "          .rename(columns={\"관측 일시\":\"date\", \"01시\":\"H01\", \"02시\":\"H02\", \"03시\":\"H03\", \"04시\":\"H04\", \"05시\":\"H05\", \"06시\":\"H06\", \"07시\":\"H07\", \"08시\":\"H08\", \"09시\":\"H09\", \"10시\":\"H10\",\"11시\":\"H11\", \"12시\":\"H12\", \"13시\":\"H13\", \"14시\":\"H14\", \"15시\":\"H15\", \"16시\":\"H16\", \"17시\":\"H17\", \"18시\":\"H18\", \"19시\":\"H19\", \"20시\":\"H20\", \"21시\":\"H21\", \"22시\":\"H22\", \"23시\":\"H23\", \"24시\":\"H24\"})\n",
        "          .drop([dataf.shape[0]-1,dataf.shape[0]-2]))\n",
        "  dataf = pd.wide_to_long(dataf, [\"H\"], i=\"date\", j=\"hour\").reset_index().rename(columns={\"H\":\"water_level\"}).sort_values(by=['date', 'hour'])\n",
        "  dataf.insert(2, 'datetime', pd.to_datetime(dataf['date'].astype(str) + ' ' +(dataf['hour']-1).astype(str) + ':00:00'))\n",
        "  return (dataf\n",
        "          .drop(columns=['hour', 'date'])\n",
        "          )\n",
        "\n",
        "def drop_bad_wl(dataf):\n",
        "  dataf['water_level']=dataf['water_level'].astype(float)\n",
        "  return (dataf\n",
        "          .replace(0.0, np.nan)\n",
        "          #.loc[(dataf['water_level']>0.0)]\n",
        "          )\n",
        "\n",
        "\n",
        "def add_station(dataf, name):\n",
        "  dataf = dataf.rename(columns={'water_level':'water_level_'+name})\n",
        "  return dataf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwKKdoiv4CaF"
      },
      "source": [
        "### Precipitations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVWBsX9r4AUE"
      },
      "outputs": [],
      "source": [
        "def prepare_df_preci(dataf):\n",
        "  dataf = (dataf\n",
        "          .rename(columns={\"관측 일시\":\"date\", \"01시\":\"H01\", \"02시\":\"H02\", \"03시\":\"H03\", \"04시\":\"H04\", \"05시\":\"H05\", \"06시\":\"H06\", \"07시\":\"H07\", \"08시\":\"H08\", \"09시\":\"H09\", \"10시\":\"H10\",\"11시\":\"H11\", \"12시\":\"H12\", \"13시\":\"H13\", \"14시\":\"H14\", \"15시\":\"H15\", \"16시\":\"H16\", \"17시\":\"H17\", \"18시\":\"H18\", \"19시\":\"H19\", \"20시\":\"H20\", \"21시\":\"H21\", \"22시\":\"H22\", \"23시\":\"H23\", \"24시\":\"H24\"})\n",
        "          .drop([dataf.shape[0]-1,dataf.shape[0]-2])\n",
        "          )\n",
        "  dataf = pd.wide_to_long(dataf, [\"H\"], i=\"date\", j=\"hour\").reset_index().rename(columns={\"H\":\"precipitation\"}).sort_values(by=['date', 'hour'])\n",
        "  dataf.insert(2, 'datetime', pd.to_datetime(dataf['date'].astype(str) + ' ' +(dataf['hour']-1).astype(str) + ':00:00'))\n",
        "  return (dataf\n",
        "          .drop(columns=['hour', 'date'])\n",
        "          )\n",
        "\n",
        "def drop_bad_preci(dataf):\n",
        "  dataf['precipitation']=dataf['precipitation'].astype(float)\n",
        "  return (dataf\n",
        "          .loc[(dataf['precipitation']>=0.0)])\n",
        "\n",
        "\n",
        "def add_station_preci(dataf, name):\n",
        "  dataf = dataf.rename(columns={'precipitation':'precipitation_'+name})\n",
        "  return dataf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480olJqC4Pz1"
      },
      "source": [
        "### Weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDWlipj54IRQ"
      },
      "outputs": [],
      "source": [
        "def prepare_df_temp(dataf):\n",
        "  dataf = dataf.drop(columns=['Unnamed: 0'])\n",
        "  dataf.insert(1, 'datetime', pd.to_datetime(df_temp['date'].astype(str)))\n",
        "  dataf = dataf.drop(columns=['date'])\n",
        "  return dataf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ekVhot4lSi"
      },
      "source": [
        "# General file preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-5hvXOK4ZgS"
      },
      "source": [
        "### File paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0l0hBz14byE"
      },
      "outputs": [],
      "source": [
        "base_drive_path = 'drive/MyDrive/flood/korea/'\n",
        "xls_path_base = 'wl/'\n",
        "wl_station = ['1007639','1007641','1007635','1007633','1007634','1007625']\n",
        "xls_path_base_preci = base_drive_path + 'preci/'\n",
        "preci_station = ['10074030_preci', '10074070_preci', '10054010_preci']\n",
        "\n",
        "xls_path_base_temp = base_drive_path + 'temp/raw_temp_clean.xlsx'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCeCTynD4o_Y"
      },
      "source": [
        "## Data Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cnt_ye_A4tKB"
      },
      "source": [
        "### Water levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWgi_-uv4vYm"
      },
      "outputs": [],
      "source": [
        "first_ts = []\n",
        "list_of_df = []\n",
        "for station_id, station_name in enumerate(wl_station):\n",
        "  xls_path = xls_path_base + station_name + '.xls'\n",
        "  df = (pd.read_excel(xls_path)\n",
        "  .pipe(prepare_df)\n",
        "  .pipe(drop_bad_wl)\n",
        "  .pipe(add_station, station_name)\n",
        "  )\n",
        "  df.reset_index(inplace=True, drop=True)\n",
        "  df = df.set_index(df['datetime'])\n",
        "  first_ts_val = df.values[0,0]\n",
        "  list_of_df.append(df)\n",
        "  first_ts.append(first_ts_val)\n",
        "\n",
        "for df_id in range(len(list_of_df)):\n",
        "  list_of_df[df_id] = list_of_df[df_id].loc[(list_of_df[df_id]['datetime']>max(first_ts))]\n",
        "  if df_id:\n",
        "    complete_df = pd.concat([complete_df, list_of_df[df_id]], axis=1)\n",
        "  else:\n",
        "    complete_df = list_of_df[df_id].copy()\n",
        "complete_df_bu = complete_df.copy()\n",
        "complete_df = complete_df.replace(r'^s*$', np.nan, regex = True)  # Replace blanks by NaN\n",
        "complete_df=complete_df.drop(columns=['datetime'])\n",
        "complete_df_inter = complete_df.copy().astype('float')\n",
        "for col_name in complete_df_inter.columns:\n",
        "  complete_df_inter[col_name] = complete_df_inter[col_name].interpolate()\n",
        "complete_df_inter.dropna(inplace=True)\n",
        "complete_df.dropna(inplace = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0EIzvhC40jW"
      },
      "source": [
        "### Weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MNfnkQI42O8"
      },
      "outputs": [],
      "source": [
        "df_temp = pd.read_excel(xls_path_base_temp)\n",
        "complete_df_temp = df_temp.pipe(prepare_df_temp)\n",
        "complete_df_temp = complete_df_temp.loc[(complete_df_temp['datetime']>=complete_df_inter.index.min())]\n",
        "complete_df_temp = complete_df_temp.loc[(complete_df_temp['datetime']<=complete_df_inter.index.max())]\n",
        "\n",
        "complete_df_temp.reset_index(inplace=True, drop=True)\n",
        "complete_df_temp = complete_df_temp.set_index(complete_df_temp['datetime'])\n",
        "complete_df_temp = complete_df_temp.drop(columns=['datetime'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws_hfG7b46aM"
      },
      "source": [
        "### Precipitations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTr7AtYL48Bl"
      },
      "outputs": [],
      "source": [
        "first_ts_preci = []\n",
        "list_of_df_preci = []\n",
        "for station_id, station_name in enumerate(preci_station):\n",
        "  xls_path_preci = xls_path_base_preci + station_name + '.xlsx'\n",
        "  df_preci = (pd.read_excel(xls_path_preci)\n",
        "    .pipe(prepare_df_preci)\n",
        "    .pipe(drop_bad_preci)\n",
        "    .pipe(add_station_preci, station_name)\n",
        "  )\n",
        "  df_preci.reset_index(inplace=True, drop=True)\n",
        "  df_preci = df_preci.set_index(df_preci['datetime'])\n",
        "  first_ts_val_preci = df_preci.values[0,0]\n",
        "  list_of_df_preci.append(df_preci.copy())\n",
        "  first_ts_preci.append(first_ts_val_preci)\n",
        "\n",
        "for df_id in range(len(list_of_df_preci)):\n",
        "  list_of_df_preci[df_id] = list_of_df_preci[df_id].loc[(list_of_df_preci[df_id]['datetime']>=(complete_df_inter.index.min()))]\n",
        "\n",
        "  if df_id:\n",
        "    complete_df_preci = pd.concat([complete_df_preci, list_of_df_preci[df_id]], axis=1)\n",
        "  else:\n",
        "    complete_df_preci = list_of_df_preci[df_id].copy()\n",
        "complete_df_preci_bu = complete_df_preci.copy()\n",
        "complete_df_preci = complete_df_preci.replace(r'^s*$', np.nan, regex = True)  # Replace blanks by NaN\n",
        "complete_df_preci= complete_df_preci.drop(columns=['datetime'])\n",
        "\n",
        "for col_name in complete_df_preci.columns:\n",
        "  complete_df_preci[col_name] = complete_df_preci[col_name].interpolate()\n",
        "\n",
        "complete_df_preci.dropna(inplace=True)\n",
        "#complete_df.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8OKG-XR5DfS"
      },
      "source": [
        "### Checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaN0eLK-48sd"
      },
      "source": [
        "#### Sizes Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0_A6DdI4-jN"
      },
      "outputs": [],
      "source": [
        "print(complete_df_preci.index.difference(complete_df_inter.index))\n",
        "print(complete_df_inter.index.difference(complete_df_preci.index))\n",
        "print(complete_df_temp.index.difference(complete_df_inter.index))\n",
        "\n",
        "print(complete_df_temp.shape)\n",
        "print(complete_df_preci.shape)\n",
        "print(complete_df_inter.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_3pY3kl5E62"
      },
      "source": [
        "#### Gaps checks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT0Y6Pht5Mtb"
      },
      "source": [
        "Water Level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiyH_OAW5XVo"
      },
      "outputs": [],
      "source": [
        "mask_inter = complete_df_inter.index.to_series().diff() > pd.Timedelta('01:00:00')\n",
        "mask_inter[mask_int_preci].index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwnxxzLS5LCp"
      },
      "source": [
        "Precipitations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rmnPl-V5J4D"
      },
      "outputs": [],
      "source": [
        "mask_int_preci = complete_df_preci.index.to_series().diff() > pd.Timedelta('01:00:00')\n",
        "mask_int_preci[mask_int_preci].index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hThfMKZ5OK3"
      },
      "source": [
        "Weather\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m53jocPj5S3t"
      },
      "outputs": [],
      "source": [
        "mask_int_temp = complete_df_temp.index.to_series().diff() > pd.Timedelta('01:00:00')\n",
        "mask_int_temp[mask_int_preci].index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk67XI8l5fAy"
      },
      "source": [
        "# Forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a63_6tTh5lZf"
      },
      "source": [
        "## General Functions Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mswg_iBP5nkP"
      },
      "outputs": [],
      "source": [
        "def compute_losses(y_train, y_fit, y_test, y_pred):\n",
        "  train_mae = mean_absolute_error(y_train, y_fit)\n",
        "  test_mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "  train_rmse = mean_squared_error(y_train, y_fit, squared=False)\n",
        "  test_rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "  def nse(targets, predictions):\n",
        "      return (1-(np.sum((targets-predictions)**2)/np.sum((targets-np.mean(targets, axis=0))**2))).mean(axis=0)\n",
        "\n",
        "  train_nse = nse(y_train, y_fit)\n",
        "  test_nse = nse(y_test, y_pred)\n",
        "\n",
        "  print((f\"Training:\\n MAE: {train_mae:.2f} MSE: {train_rmse:.2f} NSE: {train_nse:.2f}\"))\n",
        "  print((f\"Testing:\\n MAE: {test_mae:.2f} MSE: {test_rmse:.2f} NSE: {test_nse:.2f}\\n\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7msyPK7Bdmw"
      },
      "outputs": [],
      "source": [
        "def prepare_TrainTest_forecast(reframed_train, reframed_test, predict_ts, past_ts, nbr_features):\n",
        "  test = reframed_test.values\n",
        "  train = reframed_train.values\n",
        "\n",
        "  # split into input and outputs\n",
        "  train_X, train_y = train[:, :-predict_ts*nbr_features], train[:, -predict_ts*nbr_features:]\n",
        "  test_X, test_y = test[:, :-predict_ts*nbr_features], test[:, -predict_ts*nbr_features:]\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = train_X.reshape((train_X.shape[0], past_ts, nbr_features))\n",
        "  test_X = test_X.reshape((test_X.shape[0], past_ts, nbr_features))\n",
        "\n",
        "  return train_X, train_y, test_X, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWo44ik6BfrK"
      },
      "outputs": [],
      "source": [
        "def prepare_TrainTest_forecast_singleoutput(reframed_train, reframed_test, predict_ts, past_ts, nbr_features, pred_station):\n",
        "\n",
        "  # split into input and outputs\n",
        "  train_X = reframed_train.iloc[:, :(past_ts*nbr_features+1)].values\n",
        "  train_y = reframed_train.iloc[:, (past_ts*nbr_features+1):].filter(regex=pred_station).values\n",
        "\n",
        "  test_X = reframed_test.iloc[:, :(past_ts*nbr_features+1)].values\n",
        "  test_y = reframed_test.iloc[:, (past_ts*nbr_features+1):].filter(regex=pred_station).values\n",
        "\n",
        "  return train_X, train_y, test_X, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF3NSrDVBhyX"
      },
      "outputs": [],
      "source": [
        "# convert series to supervised learning\n",
        "def series_to_supervised(data_in, n_in=1, n_out=1, dropnan=True):\n",
        "  data = data_in.copy()\n",
        "  n_vars = data.shape[1]\n",
        "  cols, names = list(), list()\n",
        "  # input sequence (t-n, ... t-1)\n",
        "  for i in range(n_in, 0, -1):\n",
        "    cols.append(data.shift(i))\n",
        "    names += [('%s(t-%d)' % (data.columns[j], i)) for j in range(n_vars)]\n",
        "  # forecast sequence (t, t+1, ... t+n)\n",
        "  for i in range(0, n_out):\n",
        "    cols.append(data.shift(-i))\n",
        "    if i == 0:\n",
        "      names += [('%s(t)' % (data.columns[j])) for j in range(n_vars)]\n",
        "    else:\n",
        "      names += [('%s(t+%d)' % (data.columns[j], i)) for j in range(n_vars)]\n",
        "  # put it all together\n",
        "  agg = pd.concat(cols, axis=1)\n",
        "  agg.columns = names\n",
        "  # drop rows with NaN values\n",
        "  if dropnan:\n",
        "    agg.dropna(inplace=True)\n",
        "  return agg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOFgkn_wBjWL"
      },
      "outputs": [],
      "source": [
        "class MinMaxScale:\n",
        "\n",
        "  def __init__(self, dataf, ul=1, ll=0):\n",
        "    self.min = dataf.min()\n",
        "    self.max = dataf.max()\n",
        "    self.UL = ul\n",
        "    self.LL = ll\n",
        "\n",
        "  def NormMM(self, dataf):\n",
        "    return self.LL+(self.UL-self.LL)*(dataf - self.min)/(self.max-self.min)\n",
        "\n",
        "  def DeNormMM(self, dataf, station_filter):\n",
        "    return (dataf - self.LL) * (self.max-self.min).filter(regex=station_filter).values[0]/(self.UL-self.LL) + self.min.filter(regex=station_filter).values[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zksJdSHcBmd3"
      },
      "source": [
        "## Linear models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buC5FLhABpCB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_BUrpPSB9ja"
      },
      "source": [
        "## Classic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXJY53flBsqi"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.75\n",
        "past_ts=72\n",
        "forecast_ts_list=[2]#[i for i in range(1,13)]\n",
        "\n",
        "predict_station_list = [0]\n",
        "for predict_station_id in predict_station_list:\n",
        "  for predict_ts in forecast_ts_list:\n",
        "    predict_station = wl_station[predict_station_id]\n",
        "\n",
        "    train_ts = complete_df.iloc[int(train_ratio*complete_df.shape[0])].name\n",
        "    df_train = complete_df.loc[:train_ts].iloc[:,predict_station_id:]\n",
        "    df_test = complete_df.loc[train_ts:].iloc[:,predict_station_id:]\n",
        "    nbr_features = df_train.columns.unique().shape[0]\n",
        "\n",
        "    # define minmax scaler\n",
        "    scale_wl = MinMaxScale(df_train)\n",
        "\n",
        "    # scale the train and test data\n",
        "    df_train_scaled = scale_wl.NormMM(df_train)\n",
        "    df_test_scaled = scale_wl.NormMM(df_test)\n",
        "\n",
        "    reframed_train = series_to_supervised(df_train_scaled, past_ts, predict_ts)\n",
        "    reframed_test = series_to_supervised(df_test_scaled, past_ts, predict_ts)\n",
        "\n",
        "    train_X, train_y, test_X, test_y = prepare_TrainTest_forecast_singleoutput(reframed_train, reframed_test, predict_ts, past_ts, nbr_features,predict_station)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(train_X, train_y)\n",
        "\n",
        "    y_train_scaled = reframed_train.iloc[:, (past_ts*nbr_features+1):].filter(regex=predict_station)\n",
        "    y_test_scaled = reframed_test.iloc[:, (past_ts*nbr_features+1):].filter(regex=predict_station)\n",
        "    y_fit_scaled = pd.DataFrame(model.predict(train_X), index=y_train_scaled.index, columns=y_train_scaled.columns)\n",
        "    y_pred_scaled = pd.DataFrame(model.predict(test_X), index=y_test_scaled.index, columns=y_test_scaled.columns)\n",
        "\n",
        "    y_train = scale_wl.DeNormMM(y_train_scaled, predict_station)\n",
        "    y_test  = scale_wl.DeNormMM(y_test_scaled, predict_station)\n",
        "    y_fit   = scale_wl.DeNormMM(y_fit_scaled, predict_station)\n",
        "    y_pred  = scale_wl.DeNormMM(y_pred_scaled, predict_station)\n",
        "\n",
        "    if predict_ts > 1:\n",
        "      print(f'Prediction for station {predict_station} with a forecast of {predict_ts} hours and a lookback of {past_ts} hours.')\n",
        "    else:\n",
        "      print(f'Prediction for station {predict_station} with a forecast of {predict_ts} hour and a lookback of {past_ts} hours.')\n",
        "    compute_losses(y_train, y_fit, y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSaZfn_DBvTB"
      },
      "outputs": [],
      "source": [
        "palette = dict(palette='husl', n_colors=64)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 8))\n",
        "\n",
        "plot_window_train = [\"2015-01-01 01:00:00\", \"2015-01-02 0:00:00\"]\n",
        "plot_label_train = [\"2015-01-01 00:00:00\", \"2015-01-02 10:00:00\"]\n",
        "\n",
        "plot_window_test = [\"2022-01-01 01:00:00\", \"2022-01-02 0:00:00\"]\n",
        "plot_label_test = [\"2022-01-01 00:00:00\", \"2022-01-02 10:00:00\"]\n",
        "\n",
        "y_fit_windowed = y_fit.loc[plot_window_train[0]:plot_window_train[1]]\n",
        "y_pred_windowed = y_pred.loc[plot_window_test[0]:plot_window_test[1]]\n",
        "\n",
        "\n",
        "# Plot training prediction\n",
        "ax1= y_train.loc[plot_label_train[0]:plot_label_train[1]].iloc[:,0].plot(**plot_params, ax=ax1)\n",
        "\n",
        "if forecast_ts_list[-1] > 2:\n",
        "  ax1 = plot_multistep(y_fit_windowed, ax=ax1, palette_kwargs=palette)\n",
        "else:\n",
        "  ax1= y_fit_windowed.plot(**plot_params2, ax=ax1)\n",
        "\n",
        "ax1.title.set_text(f'Training prediction: {past_ts}H lookback and {forecast_ts_list[-1]-1}H forecast')\n",
        "ax2.title.set_text(f'Testing prediction: {past_ts}H lookback and {forecast_ts_list[-1]-1}H forecast')\n",
        "\n",
        "# Plot testing prediction\n",
        "if forecast_ts_list[-1] > 2:\n",
        "  ax2 = plot_multistep(y_pred_windowed, ax=ax2, palette_kwargs=palette)\n",
        "else:\n",
        "  ax2= y_pred_windowed.plot(**plot_params2, ax=ax2)\n",
        "\n",
        "ax2= y_test.loc[plot_label_test[0]:plot_label_test[1]].iloc[:,0].plot(**plot_params, ax=ax2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gHScQT1CAiN"
      },
      "source": [
        "## L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfEmhNUXB3La"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.75\n",
        "past_ts=72\n",
        "forecast_ts_list=[2]#[i for i in range(1,13)]\n",
        "\n",
        "predict_station_list = [0]\n",
        "for predict_station_id in predict_station_list:\n",
        "  for predict_ts in forecast_ts_list:\n",
        "    predict_station = wl_station[predict_station_id]\n",
        "\n",
        "    train_ts = complete_df.iloc[int(train_ratio*complete_df.shape[0])].name\n",
        "    df_train = complete_df.loc[:train_ts].iloc[:,predict_station_id:]\n",
        "    df_test = complete_df.loc[train_ts:].iloc[:,predict_station_id:]\n",
        "    nbr_features = df_train.columns.unique().shape[0]\n",
        "\n",
        "    # define minmax scaler\n",
        "    scale_wl = MinMaxScale(df_train)\n",
        "\n",
        "    # scale the train and test data\n",
        "    df_train_scaled = scale_wl.NormMM(df_train)\n",
        "    df_test_scaled = scale_wl.NormMM(df_test)\n",
        "\n",
        "    reframed_train = series_to_supervised(df_train_scaled, past_ts, predict_ts)\n",
        "    reframed_test = series_to_supervised(df_test_scaled, past_ts, predict_ts)\n",
        "\n",
        "    train_X, train_y, test_X, test_y = prepare_TrainTest_forecast_singleoutput(reframed_train, reframed_test, predict_ts, past_ts, nbr_features,predict_station)\n",
        "\n",
        "    ridge = Ridge()\n",
        "    ridge.fit(train_X, train_y)\n",
        "\n",
        "    y_train_scaled = reframed_train.iloc[:, (past_ts*nbr_features+1):].filter(regex=predict_station)\n",
        "    y_test_scaled = reframed_test.iloc[:, (past_ts*nbr_features+1):].filter(regex=predict_station)\n",
        "    y_fit_scaled = pd.DataFrame(ridge.predict(train_X), index=y_train_scaled.index, columns=y_train_scaled.columns)\n",
        "    y_pred_scaled = pd.DataFrame(ridge.predict(test_X), index=y_test_scaled.index, columns=y_test_scaled.columns)\n",
        "\n",
        "    y_train = scale_wl.DeNormMM(y_train_scaled, predict_station)\n",
        "    y_test  = scale_wl.DeNormMM(y_test_scaled, predict_station)\n",
        "    y_fit   = scale_wl.DeNormMM(y_fit_scaled, predict_station)\n",
        "    y_pred  = scale_wl.DeNormMM(y_pred_scaled, predict_station)\n",
        "\n",
        "    if predict_ts > 1:\n",
        "      print(f'Prediction for station {predict_station} with a forecast of {predict_ts} hours and a lookback of {past_ts} hours.')\n",
        "    else:\n",
        "      print(f'Prediction for station {predict_station} with a forecast of {predict_ts} hour and a lookback of {past_ts} hours.')\n",
        "    compute_losses(y_train, y_fit, y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RtcX830B6is"
      },
      "outputs": [],
      "source": [
        "palette = dict(palette='husl', n_colors=64)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 8))\n",
        "\n",
        "plot_window_train = [\"2015-01-01 01:00:00\", \"2015-01-02 0:00:00\"]\n",
        "plot_label_train = [\"2015-01-01 00:00:00\", \"2015-01-02 10:00:00\"]\n",
        "\n",
        "plot_window_test = [\"2022-01-01 01:00:00\", \"2022-01-02 0:00:00\"]\n",
        "plot_label_test = [\"2022-01-01 00:00:00\", \"2022-01-02 10:00:00\"]\n",
        "\n",
        "y_fit_windowed = y_fit.loc[plot_window_train[0]:plot_window_train[1]]\n",
        "y_pred_windowed = y_pred.loc[plot_window_test[0]:plot_window_test[1]]\n",
        "\n",
        "\n",
        "# Plot training prediction\n",
        "ax1= y_train.loc[plot_label_train[0]:plot_label_train[1]].iloc[:,0].plot(**plot_params, ax=ax1)\n",
        "\n",
        "if forecast_ts_list[-1] > 2:\n",
        "  ax1 = plot_multistep(y_fit_windowed, ax=ax1, palette_kwargs=palette)\n",
        "else:\n",
        "  ax1= y_fit_windowed.plot(**plot_params2, ax=ax1)\n",
        "\n",
        "ax1.title.set_text(f'Training prediction: {past_ts}H lookback and {forecast_ts_list[-1]-1}H forecast')\n",
        "ax2.title.set_text(f'Testing prediction: {past_ts}H lookback and {forecast_ts_list[-1]-1}H forecast')\n",
        "\n",
        "# Plot testing prediction\n",
        "if forecast_ts_list[-1] > 2:\n",
        "  ax2 = plot_multistep(y_pred_windowed, ax=ax2, palette_kwargs=palette)\n",
        "else:\n",
        "  ax2= y_pred_windowed.plot(**plot_params2, ax=ax2)\n",
        "\n",
        "ax2= y_test.loc[plot_label_test[0]:plot_label_test[1]].iloc[:,0].plot(**plot_params, ax=ax2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgPcssKC5glt"
      },
      "source": [
        "## LSTM Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-I79dd4C7PE"
      },
      "source": [
        "## General functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyvQZsb1DZlM"
      },
      "outputs": [],
      "source": [
        "class FLSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=256, num_layers=2, forecast_length = 1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, forecast_length)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PhDL6S7Dg7e"
      },
      "outputs": [],
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "    print(tag_scores)\n",
        "\n",
        "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
        "    # for word i. The predicted tag is the maximum scoring tag.\n",
        "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "    # since 0 is index of the maximum value of row 1,\n",
        "    # 1 is the index of maximum value of row 2, etc.\n",
        "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "    print(tag_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVuinn72E76e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def define_model(train_X, train_y, input_size=1, hidden_size=256, num_layers=2, forecast_length = 1, lr=0.001, batch_size = 20)\n",
        "  model = FLSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, forecast_length = forecast_length)\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  # Create dataloader\n",
        "  training_loader = data.DataLoader(data.TensorDataset(train_X, train_y), shuffle=True, batch_size=batch_size)\n",
        "\n",
        "  return model, loss_function, optimizer, training_loader\n",
        "\n",
        "\n",
        "\n",
        "def training_model(model, loss_function, optimizer, training_loader, train_X, train_y, test_X, test_y, nbr_epoch = 30, save_epoch = 10, save_path=None)\n",
        "  for epoch in range(nbr_epoch):\n",
        "      model.train()\n",
        "      for X_batch, y_batch in training_loader:\n",
        "          y_fit = model(X_batch)\n",
        "          loss = loss_function(y_fit, y_batch)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      # Saves model\n",
        "      if not (epoch + 1) % save_epoch and save_path is not None:\n",
        "        savename_path = base_drive_path + 'weights/' + save_path + '_epoch_'+str(epoch+1)+'.pt'\n",
        "        torch.save(model.state_dict(), savename_path) # official recommended\n",
        "\n",
        "      # Validation\n",
        "      if (epoch+1) % 5 != 0:\n",
        "          continue\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          y_fit = model(train_X)\n",
        "          train_rmse = np.sqrt(loss_fn(y_fit, train_y))\n",
        "          y_pred = model(test_X)\n",
        "          test_rmse = np.sqrt(loss_fn(y_pred, test_y))\n",
        "      print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
        "  return y_fit, y_pred, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWPLWZ_0C8rm"
      },
      "outputs": [],
      "source": [
        "def prepare_TrainTest_forecast_singleoutput_LSTM(reframed_train, reframed_test, predict_ts, past_ts, nbr_features, pred_station):\n",
        "\n",
        "  # split into input and outputs\n",
        "  train_X = reframed_train.iloc[:, :(past_ts+1)*nbr_features].values\n",
        "  train_y = reframed_train.iloc[:, (past_ts+1)*nbr_features:].filter(regex=pred_station).values\n",
        "\n",
        "  test_X = reframed_test.iloc[:, :(past_ts+1)*nbr_features].values\n",
        "  test_y = reframed_test.iloc[:, (past_ts+1)*nbr_features:].filter(regex=pred_station).values\n",
        "\n",
        "\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = train_X.reshape((train_X.shape[0], past_ts+1, nbr_features))\n",
        "  test_X = test_X.reshape((test_X.shape[0], past_ts+1, nbr_features))\n",
        "\n",
        "  return train_X, train_y, test_X, test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "getYz9KjC_Vw"
      },
      "outputs": [],
      "source": [
        "def prepare_past_stages(reframed_train, reframed_test, predict_ts, past_ts, nbr_features, pred_station, predict_station_list):\n",
        "\n",
        "  # split into input and outputs\n",
        "  train_X = reframed_train.iloc[:, :(past_ts+1)*nbr_features]\n",
        "  train_y = reframed_train.iloc[:, (past_ts+1)*nbr_features:]\n",
        "\n",
        "  test_X = reframed_test.iloc[:, :(past_ts+1)*nbr_features]\n",
        "  test_y = reframed_test.iloc[:, (past_ts+1)*nbr_features:]\n",
        "\n",
        "  if predict_station_list is not None:\n",
        "    past_train_x, past_test_x, past_train_y, past_test_y = None, None, None, None\n",
        "    for id_filter in predict_station_list:\n",
        "\n",
        "      tmp_train_x = (train_X.filter(regex=id_filter)).values\n",
        "      tmp_train_y = (train_y.filter(regex=id_filter)).values\n",
        "\n",
        "      tmp_test_x = (test_X.filter(regex=id_filter)).values\n",
        "      tmp_test_y = (test_y.filter(regex=id_filter)).values\n",
        "\n",
        "      if past_train_x is None:\n",
        "        past_train_x = tmp_train_x.copy()\n",
        "        past_train_y = tmp_train_y.copy()\n",
        "        past_test_x = tmp_test_x.copy()\n",
        "        past_test_y = tmp_test_y.copy()\n",
        "\n",
        "      else:\n",
        "        past_train_x = np.concatenate((past_train_x,tmp_train_x), axis=1)\n",
        "        past_train_y = np.concatenate((past_train_y,tmp_train_y), axis=1)\n",
        "        past_test_x = np.concatenate((past_test_x,tmp_test_x), axis=1)\n",
        "        past_test_y = np.concatenate((past_test_y,tmp_test_y), axis=1)\n",
        "\n",
        "    nbr_features = len(predict_station_list)+1\n",
        "\n",
        "\n",
        "  else:\n",
        "    past_train_x = train_X.drop(train_X.filter(regex=pred_station).columns,axis=1).values\n",
        "    past_train_y = train_y.drop(train_y.filter(regex=pred_station).columns,axis=1).values\n",
        "\n",
        "    past_test_x = test_X.drop(test_X.filter(regex=pred_station).columns,axis=1).values\n",
        "    past_test_y = test_y.drop(test_y.filter(regex=pred_station).columns,axis=1).values\n",
        "\n",
        "  train_X = train_X.filter(regex=pred_station).values\n",
        "  train_y = train_y.filter(regex=pred_station).values\n",
        "  test_X_pd = test_X.filter(regex=pred_station)\n",
        "  test_X = test_X_pd.values\n",
        "  test_y = test_y.filter(regex=pred_station).values\n",
        "\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "\n",
        "  train_X = train_X.reshape((train_X.shape[0], past_ts+1, 1))\n",
        "  past_train_x = past_train_x.reshape((past_train_x.shape[0], past_ts+1, nbr_features-1))\n",
        "\n",
        "  test_X = test_X.reshape((test_X.shape[0], past_ts+1, 1))\n",
        "  past_test_x = past_test_x.reshape((past_test_x.shape[0], past_ts+1, nbr_features-1))\n",
        "\n",
        "\n",
        "  return train_X, train_y, test_X, test_y, past_train_x, past_train_y, past_test_x, past_test_y, test_X_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAgZ0HSWK6rY"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "# TODO complete this for correct prediction\n",
        "train_ratio = 0.75\n",
        "past_ts=19\n",
        "forecast_ts_list=[2]#[i for i in range(1,13)]\n",
        "train = True\n",
        "norm_input = True\n",
        "load_model = False\n",
        "\n",
        "df_fused = pd.concat([complete_df_inter, complete_df_preci, complete_df_temp], axis=1)\n",
        "\n",
        "fuse_input = True\n",
        "other_station_id = None\n",
        "predict_station_list = [0, 1, 2, 3]\n",
        "for predict_station_id in predict_station_list:\n",
        "  for predict_ts in forecast_ts_list:\n",
        "    predict_station = wl_station[predict_station_id]\n",
        "\n",
        "    if fuse_input:\n",
        "      other_input_station = predict_station_list.copy()\n",
        "      other_input_station.pop(predict_station_id)\n",
        "\n",
        "      other_station_id = [wl_station[i] for i in other_input_station]\n",
        "\n",
        "      other_station_id += preci_station\n",
        "    train_ts = df_fused.iloc[int(train_ratio*df_fused.shape[0])].name\n",
        "    df_train = df_fused.loc[:train_ts].iloc[:,predict_station_id:]\n",
        "    df_test = df_fused.loc[train_ts:].iloc[:,predict_station_id:]\n",
        "    nbr_features = df_train.columns.unique().shape[0]\n",
        "\n",
        "\n",
        "    if norm_input:\n",
        "      # define minmax scaler\n",
        "      scale_wl = MinMaxScale(df_train, ul=1, ll=-1)\n",
        "\n",
        "      # scale the train and test data\n",
        "      df_train_scaled = scale_wl.NormMM(df_train)\n",
        "      df_test_scaled = scale_wl.NormMM(df_test)\n",
        "    else:\n",
        "      df_train_scaled = df_train\n",
        "      df_test_scaled = df_test\n",
        "\n",
        "    reframed_train = series_to_supervised(df_train_scaled, past_ts, predict_ts)#.diff()[1:]\n",
        "    reframed_test = series_to_supervised(df_test_scaled, past_ts, predict_ts)#.diff()[1:]\n",
        "    train_X, train_y, test_X, test_y, past_train_x, past_train_y, past_test_x, past_test_y, test_X_pd = prepare_past_stages(reframed_train, reframed_test, predict_ts, past_ts, nbr_features, predict_station, other_station_id)\n",
        "\n",
        "    if fuse_input:\n",
        "      train_X = np.concatenate((train_X,past_train_x), axis=2)\n",
        "      test_X = np.concatenate((test_X,past_test_x), axis=2)\n",
        "\n",
        "    y_train_scaled = reframed_train.iloc[:, (past_ts*nbr_features+1):].filter(regex=predict_station)\n",
        "    y_test_scaled = reframed_test.iloc[:, (past_ts*nbr_features+1):].filter(regex=predict_station)\n",
        "\n",
        "    if train or load_model:\n",
        "      model, loss_function, optimizer, training_loader = define_model(train_X, train_y, input_size=1, hidden_size=256, num_layers=2, forecast_length = predict_ts-1, lr=0.001, batch_size = 20)\n",
        "\n",
        "      if load_model:\n",
        "        model.load_state_dict(torch.load(base_drive_path + 'weights/training_model_epoch_20_300_hl.pt'))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            y_fit = model(train_X)\n",
        "            y_pred = model(test_X)\n",
        "      if train:\n",
        "        savename = 'interpolated_preci_weather_one_forecast_two_input_training_model'\n",
        "        y_fit, y_pred, model = training_model(model, loss_function, optimizer, training_loader, train_X, train_y, test_X, test_y, nbr_epoch = 30, save_epoch = 10, save_path=savename)\n",
        "\n",
        "      y_fit_scaled = pd.DataFrame(y_fit, index=y_train_scaled.index, columns=y_train_scaled.columns)\n",
        "      y_pred_scaled = pd.DataFrame(y_pred, index=y_test_scaled.index, columns=y_test_scaled.columns)\n",
        "      if norm_input:\n",
        "        y_fit   = scale_wl.DeNormMM(y_fit_scaled, predict_station)\n",
        "        y_pred  = scale_wl.DeNormMM(y_pred_scaled, predict_station)\n",
        "      else:\n",
        "        y_fit = y_fit_scaled\n",
        "        y_pred = y_pred_scaled\n",
        "\n",
        "    if norm_input:\n",
        "      y_train = scale_wl.DeNormMM(y_train_scaled, predict_station)\n",
        "      y_test  = scale_wl.DeNormMM(y_test_scaled, predict_station)\n",
        "    else:\n",
        "      y_train = y_train_scaled\n",
        "      y_test = y_test_scaled\n",
        "\n",
        "    if train:\n",
        "      if predict_ts > 1:\n",
        "        print(f'Prediction for station {predict_station} with a forecast of {predict_ts-1} hours and a lookback of {past_ts} hours.')\n",
        "      else:\n",
        "        print(f'Prediction for station {predict_station} with a forecast of {predict_ts-1} hour and a lookback of {past_ts} hours.')\n",
        "      compute_losses(y_train, y_fit, y_test, y_pred)\n",
        "\n",
        "  break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_u7zaCnT4hge",
        "R_-YAUHx39YL",
        "oicsLFZr4EJi",
        "mwKKdoiv4CaF",
        "zksJdSHcBmd3",
        "U_BUrpPSB9ja",
        "2gHScQT1CAiN"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
